{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4f49ae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+-----------+------+----+----+--------------+--------------+\n",
      "|product|location|stock_start|demand|week|days|sales_forecast|stock_forecast|\n",
      "+-------+--------+-----------+------+----+----+--------------+--------------+\n",
      "|      1|      01|       1000|   100|  22|   4|           400|           600|\n",
      "|      1|      01|        600|   100|  23|   7|           600|             0|\n",
      "|      1|      01|          0|   100|  24|   7|             0|             0|\n",
      "|      1|      01|          0|   100|  25|   7|             0|             0|\n",
      "|      1|      01|          0|   100|  26|   5|             0|             0|\n",
      "|      1|      02|        400|   110|  22|   4|           400|             0|\n",
      "|      1|      02|          0|   110|  23|   7|             0|             0|\n",
      "|      1|      02|          0|   110|  24|   7|             0|             0|\n",
      "|      1|      02|          0|   110|  25|   7|             0|             0|\n",
      "|      1|      02|          0|   110|  26|   5|             0|             0|\n",
      "|      2|      01|        300|   120|  22|   4|           300|             0|\n",
      "|      2|      01|          0|   120|  23|   7|             0|             0|\n",
      "|      2|      01|          0|   120|  24|   7|             0|             0|\n",
      "|      2|      01|          0|   120|  25|   7|             0|             0|\n",
      "|      2|      01|          0|   120|  26|   5|             0|             0|\n",
      "|      2|      02|        250|    90|  22|   4|           250|             0|\n",
      "|      2|      02|          0|    90|  23|   7|             0|             0|\n",
      "|      2|      02|          0|    90|  24|   7|             0|             0|\n",
      "|      2|      02|          0|    90|  25|   7|             0|             0|\n",
      "|      2|      02|          0|    90|  26|   5|             0|             0|\n",
      "|      3|      01|          0|    70|  22|   4|             0|             0|\n",
      "|      3|      01|          0|    70|  23|   7|             0|             0|\n",
      "|      3|      01|          0|    70|  24|   7|             0|             0|\n",
      "|      3|      01|          0|    70|  25|   7|             0|             0|\n",
      "|      3|      01|          0|    70|  26|   5|             0|             0|\n",
      "|      3|      02|          0|    80|  22|   4|             0|             0|\n",
      "|      3|      02|          0|    80|  23|   7|             0|             0|\n",
      "|      3|      02|          0|    80|  24|   7|             0|             0|\n",
      "|      3|      02|          0|    80|  25|   7|             0|             0|\n",
      "|      3|      02|          0|    80|  26|   5|             0|             0|\n",
      "+-------+--------+-----------+------+----+----+--------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from datetime import timedelta as td\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType\n",
    "from pyspark.sql.types import TimestampType, StringType\n",
    "\n",
    "\n",
    "# Инициализируем Spark-сессию\n",
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName(\"SparkFirst\")\\\n",
    "        .config(\"spark.executor.memory\", \"6G\")\\\n",
    "        .config(\"spark.executor.cores\", 6)\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Задача 1. Для каждого id рассчитайте усредненную длину сессии в рамках суток:\n",
    "\n",
    "data = [\n",
    "    (1, dt.fromtimestamp(1562007679)), (1, dt.fromtimestamp(1562007710)),\n",
    "    (1, dt.fromtimestamp(1562007720)), (1, dt.fromtimestamp(1562007750)),\n",
    "    (2, dt.fromtimestamp(1564682430)), (2, dt.fromtimestamp(1564682450)),\n",
    "    (2, dt.fromtimestamp(1564682480))\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "w = Window.partitionBy('id', 'date')\n",
    "w2 = Window.partitionBy('id')\n",
    "\n",
    "df = spark.createDataFrame(data=data, schema=schema)\\\n",
    "     .withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\\\n",
    "     .withColumn(\n",
    "        \"session_len\",\n",
    "        (F.max(F.col(\"timestamp\").cast(\"long\")).over(w)\n",
    "         - F.min(F.col(\"timestamp\").cast(\"long\")).over(w))\n",
    "      )\n",
    "\n",
    "# Задача 2. Необходимо сформировать витрину данных, в которой для каждой пары\n",
    "#           товар-локация на уровне каждой технической недели будет рассчитано\n",
    "#           прогнозируемое значение количества проданных товаров (с учетом\n",
    "#           среднедневного спроса) и количество остатков товара на складе:\n",
    "\n",
    "# создадим df с данными о среднедневном спросе\n",
    "data = [\n",
    "    (1, '01', 100), (1, '02', 110), (2, '01', 120),\n",
    "    (2, '02', 90), (3, '01', 70), (3, '02', 80)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product\", IntegerType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"demand\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "df_demand = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# теперь создадим df с остатками\n",
    "data = [\n",
    "    (1, '01', 1000), (1, '02', 400),\n",
    "    (2, '01', 300), (2, '02', 250)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"product\", IntegerType(), False),\n",
    "    StructField(\"location\", StringType(), False),\n",
    "    StructField(\"stock\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "df_stock = spark.createDataFrame(data=data, schema=schema)\n",
    "\n",
    "# сделаем отдельный df c неделями и днями за июнь 2023 по условию задачи\n",
    "start_date = dt.strptime('01.06.2023', '%d.%m.%Y')\n",
    "end_date = dt.strptime('30.06.2023', '%d.%m.%Y')\n",
    "\n",
    "weeks = []\n",
    "while start_date <= end_date:\n",
    "    weeks.append((start_date.isocalendar().week, 1))\n",
    "    start_date += td(days=1)\n",
    "\n",
    "df_weeks = spark.createDataFrame(\n",
    "    data=weeks,\n",
    "    schema=['week', 'day']\n",
    ").groupby('week').agg(F.sum('day').alias('days'))\n",
    "\n",
    "# сделаем декартово произведение и рассчитаем прогноз продаж\n",
    "df_join = df_demand.crossJoin(df_weeks)\\\n",
    "                   .withColumn(\n",
    "                    'sales_forecast', F.col('demand') * F.col('days')\n",
    "                   ).orderBy('week', 'location', 'product')\n",
    "\n",
    "# теперь итеративно рассчитаем сток от недели к неделе\n",
    "weeks = list(df_weeks.toPandas()['week'])\n",
    "\n",
    "# первая неделя отдельно. Присоединим начальный сток\n",
    "df_result = df_join.join(\n",
    "        df_stock, [\n",
    "            df_stock.product == df_demand.product,\n",
    "            df_stock.location == df_demand.location\n",
    "        ],\n",
    "        'left'\n",
    "    ).select(\n",
    "        df_join.product, df_join.location, df_stock.stock.alias('stock_start'),\n",
    "        df_join.demand, df_join.week, df_join.days, df_join.sales_forecast\n",
    "    ).na.fill(0).filter(F.col('week') == weeks[0]).withColumn(\n",
    "        'stock_forecast', F.col('stock_start') - F.col('sales_forecast')\n",
    "    )\n",
    "\n",
    "# далее остальные\n",
    "for week in weeks[1:]:\n",
    "    df_join_iter = df_join.filter(F.col('week') == week)\n",
    "    df_result_iter = df_result.filter(F.col('week') == week - 1)\n",
    "\n",
    "    df_result = df_result.union(\n",
    "        df_join_iter.join(\n",
    "            df_result_iter, [\n",
    "                df_join_iter.product == df_result_iter.product,\n",
    "                df_join_iter.location == df_result_iter.location\n",
    "            ]\n",
    "        ).select(\n",
    "            df_join_iter.product, df_join_iter.location,\n",
    "            df_result_iter.stock_forecast.alias('stock_start'),\n",
    "            df_join_iter.demand, df_join_iter.week,\n",
    "            df_join_iter.days, df_join_iter.sales_forecast\n",
    "        ).withColumn(\n",
    "            'stock_forecast', F.col('stock_start') - F.col('sales_forecast')\n",
    "        )\n",
    "    )\n",
    "\n",
    "# финальный вывод, откорректируем расчёты.\n",
    "df_result.select(\n",
    "    F.col('product'),\n",
    "    F.col('location'),\n",
    "    # стартовый сток не меньше нуля\n",
    "    F.greatest(F.col('stock_start'), F.lit(0)).alias('stock_start'),\n",
    "    F.col('demand'),\n",
    "    F.col('week'),\n",
    "    F.col('days'),\n",
    "    # продажи не больше, чем стока в наличии\n",
    "    (F.greatest(F.col('stock_start'), F.lit(0))\n",
    "     - F.greatest(F.col('stock_forecast'), F.lit(0))\n",
    "    ).alias('sales_forecast'),\n",
    "    # прогнозируемый сток на конец недели не меньше нуля\n",
    "    F.greatest(F.col('stock_forecast'), F.lit(0)).alias('stock_forecast')\n",
    ").orderBy('product', 'location', 'week').show(50)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61e47dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
